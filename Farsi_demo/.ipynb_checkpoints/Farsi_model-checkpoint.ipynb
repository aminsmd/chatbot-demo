{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adel/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports \n",
    "#from tensorflow import keras\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM,Dense,Input,Bidirectional\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from scipy import spatial\n",
    "from random import shuffle\n",
    "import pickle as pk\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from scipy import spatial\n",
    "import csv\n",
    "import requests\n",
    "import json\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Word Embedding</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "embeddings_size = 300\n",
    "i = 0\n",
    "with open('/home/adel/Downloads/chatBot/cc.fa.300.vec') as f:\n",
    "    for line in f:\n",
    "        if i > 400000:\n",
    "            break\n",
    "        i += 1\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = [float(i) for i in values[1:]]\n",
    "        embeddings_index[word] = coefs\n",
    "embeddings_index['<PAD>'] = [0] * 300\n",
    "embeddings_index['<UNK>'] = [1] * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"farsi embeddings\",\"wb\") as f:\n",
    "    pk.dump(embeddings_index , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(dataSetI , dataSetII):\n",
    "    return 1 - spatial.distance.cosine(dataSetI, dataSetII)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Preparing our Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "marks = {'؟', '!', '.', '،'}\n",
    "zamir_1 = {'م' , 'ش', 'ت', 'ه','و','ی'} \n",
    "zamir_3 = {'تان', 'شان', 'مان'}\n",
    "def prepareSent(sent):\n",
    "    sent = sent.split()\n",
    "    sent = [i[:-1] if i[-1] in marks or (i[-1] in zamir_1 and i[:-1] in embeddings_index and i not in embeddings_index) else i for i in sent]\n",
    "    sent = [i[:-3] if i[-3:] in zamir_3 and i[:-3] in embeddings_index and i not in embeddings_index else i for i in sent]\n",
    "    tokenized = []\n",
    "    n = 0\n",
    "    while(n != len(sent)):\n",
    "        if n < len(sent) - 2 and (sent[n] + \"‌\" + sent[n+1] + \"‌\" + sent[n+2]) in embeddings_index:\n",
    "            tokenized.append(sent[n] + \"‌\" + sent[n+1] + \"‌\" + sent[n+2])\n",
    "            n += 2\n",
    "        elif n != len(sent) -1 and (sent[n] + \"‌\" + sent[n+1]) in embeddings_index:\n",
    "            tokenized.append(sent[n] + \"‌\" + sent[n+1])\n",
    "            n += 1\n",
    "        else:\n",
    "            tokenized.append(sent[n])\n",
    "        n += 1\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_size = 5\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for i in range(1,class_size+1):\n",
    "    with open('class{}.csv'.format(i)) as f:\n",
    "        reader = csv.reader(f)\n",
    "        class_list = [r[0] for r in reader]\n",
    "        y = [1 if j == i else 0 for j in range(1 , class_size+1)]\n",
    "        class_list = [(o,y) for o in class_list]\n",
    "        X_train = X_train + class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(X_train)\n",
    "shuffle(X_train)\n",
    "shuffle(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = [i[1] for i in X_train]\n",
    "X_train = [prepareSent(i[0]) for i in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ = 20\n",
    "def wordToVec(data):\n",
    "    for s in range(len(data)):\n",
    "        n = MAX_SEQ - len(data[s])\n",
    "        if n < 0:\n",
    "            data[s] = data[s][:MAX_SEQ]\n",
    "        else:\n",
    "            for i in range(n):\n",
    "                data[s].append('<PAD>')\n",
    "        for v in range(len(data[s])):\n",
    "            if data[s][v] not in embeddings_index:\n",
    "                data[s][v] = embeddings_index['<UNK>']\n",
    "            else:\n",
    "                data[s][v] = embeddings_index[data[s][v]]\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(503, 20, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = wordToVec(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Building model with Batch size 64</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 20, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 40)                51360     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 205       \n",
      "=================================================================\n",
      "Total params: 51,565\n",
      "Trainable params: 51,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "input_layer = Input( batch_shape = (None, MAX_SEQ, 300))\n",
    "lstm_layer = Bidirectional(LSTM(units=MAX_SEQ))(input_layer)\n",
    "output_layer = Dense(class_size, activation=\"softmax\")(lstm_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:500]\n",
    "Y_train = np.array(Y_train[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('weight_farsi_dekhte.{epoch:02d}.hdf5', period=25)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('weight_farsi_dekhte.50.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "EPOCH_SIZE = 50\n",
    "model.fit(X_train, Y_train, epochs=EPOCH_SIZE, batch_size=BATCH_SIZE, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining entities\n",
    "embeddings_size = 300\n",
    "entity_lists = { \"cloth\" : ['شلوار' , 'کاپشن' , 'پیراهن'],\n",
    "                \"city_iran\" : ['شیراز', 'کرج', 'مشهد'],\n",
    "                \"food\" : ['کباب', 'ساندویچ', 'سوپ'],\n",
    "                \"time\" : ['امروز', 'فردا', 'دیروز']\n",
    "               }\n",
    "for ent in entity_lists:\n",
    "    sum_of_embedding = np.zeros(embeddings_size)\n",
    "    for obj in entity_lists[ent]:\n",
    "        sum_of_embedding += np.array(embeddings_index[obj])\n",
    "    sum_of_embedding /= len(entity_lists[ent])\n",
    "    globals()['entity_{}'.format(ent)] = list(sum_of_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recognize entity\n",
    "def return_entity(sent , entity):\n",
    "    sent = sent.lower()\n",
    "    ma = 0\n",
    "    ans = \"\"\n",
    "    for i in sent.split():\n",
    "        if sim(embeddings_index[i] , entity) > ma:\n",
    "            ma = sim(embeddings_index[i] , entity)\n",
    "            ans = i\n",
    "    if ma < .35:\n",
    "        return \"nothing\"\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def address(sent):\n",
    "    sent = sent.replace(' ','+')\n",
    "    res = requests.get(\"https://www.google.com/maps/search/?api=1&query={}+اصفهان&hl=fa\".format(sent))\n",
    "    print(res.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def food(sent):\n",
    "    menu = ['منو' , 'فهرست' , 'منوی', 'لیست' ]\n",
    "    spl = sent.split()\n",
    "    for i in menu:\n",
    "        if i in spl:\n",
    "            print('! الآن منو رو برات میفرستم')\n",
    "            return\n",
    "    print('الآن سفارشتونو به همکارام میگم')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather(sent):\n",
    "    city = return_entity(sent, entity_city_iran)\n",
    "    if city == 'nothing':\n",
    "        city = 'اصفهان'\n",
    "    blob = TextBlob(city)\n",
    "    blob = blob.translate(to=\"en\")\n",
    "    res = requests.get(\"https://api.openweathermap.org/data/2.5/weather?q={}&appid=3bb1d3931ea6593a0833bd5cf0b97ac3&lang=fa\".format(blob))\n",
    "    if res.status_code == 200:\n",
    "        data = json.loads(res.text)\n",
    "        temp = int(data['main']['temp'] - 273.15)\n",
    "        desc = data['weather'][0]['description']\n",
    "        blob = TextBlob(desc)\n",
    "        print('{} : {}'.format(city, desc))\n",
    "        print('و دمای هوا {} درجه سانتیگراد می‌باشد'.format(temp))\n",
    "    else:\n",
    "        print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whichClass(inp):\n",
    "    for i in range(len(inp)):\n",
    "        if 1 == inp[i]:\n",
    "            if i == 0:\n",
    "                return \"Address\"\n",
    "            elif i == 1:\n",
    "                return \"Restaurant\"\n",
    "            elif i == 2:\n",
    "                print(\"الآن به همکارام تو خانه‌داری اطلاع میدم\")\n",
    "            elif i == 3:\n",
    "                print(\"الآن به همکارام تو خشک‌شویی اطلاع میدم\")\n",
    "            elif i == 4:\n",
    "                return \"Weather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(sent):\n",
    "    sentence = prepareSent(sent)\n",
    "    sent = ' '.join(list(sentence))\n",
    "    sentence = wordToVec([sentence])[0]\n",
    "    sentence = np.reshape(sentence , (1 , 20 , 300))\n",
    "    \n",
    "    sentence = model.predict(sentence)\n",
    "    argmax = np.argmax(sentence)\n",
    "    a = [1 if argmax == i else 0 for i in range(class_size)]\n",
    "    res = whichClass(list(a))\n",
    "    if res == 'Address':\n",
    "        address(sent)\n",
    "    elif res == 'Weather':\n",
    "        weather(sent)\n",
    "    elif res == 'Restaurant':\n",
    "        food(sent)\n",
    "    else:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/maps/search/?api=1&query=%D8%B1%D8%B3%D8%AA%D9%88%D8%B1%D8%A7%D9%86+%D8%AE%D9%88%D8%A8+%D8%AA%D9%88+%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86+%D9%87%D8%B3%D8%AA+%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86&hl=fa\n"
     ]
    }
   ],
   "source": [
    "sent = 'رستوران خوب تو اصفهان هست؟'\n",
    "classify(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اصفهان : کمی ابری\n",
      "و دمای هوا 37 درجه سانتیگراد می‌باشد\n"
     ]
    }
   ],
   "source": [
    "sent = 'فردا بارون میاد؟'\n",
    "classify(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! الآن منو رو برات میفرستم\n"
     ]
    }
   ],
   "source": [
    "sent = 'منوی کافی شاپ رو برام بیار'\n",
    "classify(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الآن به همکارام تو خانه‌داری اطلاع میدم\n"
     ]
    }
   ],
   "source": [
    "sent = 'تختم خیلی صدا میده'\n",
    "classify(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الآن به همکارام تو خشک‌شویی اطلاع میدم\n"
     ]
    }
   ],
   "source": [
    "sent = 'چندتا لباس داشتم میخواستم برام بشورین'\n",
    "classify(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
