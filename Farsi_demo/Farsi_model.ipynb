{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "#from tensorflow import keras\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM,Dense,Input,Bidirectional\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from scipy import spatial\n",
    "from random import shuffle\n",
    "import pickle\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from scipy import spatial\n",
    "import csv\n",
    "import requests\n",
    "import json\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Word Embedding</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "embeddings_size = 300\n",
    "i = 0\n",
    "with open('/home/adel/Downloads/chatBot/cc.fa.300.vec') as f:\n",
    "    for line in f:\n",
    "        if i > 400000:\n",
    "            break\n",
    "        i += 1\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = [float(i) for i in values[1:]]\n",
    "        embeddings_index[word] = coefs\n",
    "embeddings_index['<PAD>'] = [0] * 300\n",
    "embeddings_index['<UNK>'] = [1] * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(dataSetI , dataSetII):\n",
    "    return 1 - spatial.distance.cosine(dataSetI, dataSetII)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Preparing our Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "marks = {'؟', '!', '.', '،'}\n",
    "zamir_1 = {'م' , 'ش', 'ت', 'ه','و','ی'} \n",
    "zamir_3 = {'تان', 'شان', 'مان'}\n",
    "def prepareSent(sent):\n",
    "    sent = sent.split()\n",
    "    sent = [i[:-1] if i[-1] in marks or (i[-1] in zamir_1 and i[:-1] in embeddings_index and i not in embeddings_index) else i for i in sent]\n",
    "    sent = [i[:-3] if i[-3:] in zamir_3 and i[:-3] in embeddings_index and i not in embeddings_index else i for i in sent]\n",
    "    tokenized = []\n",
    "    n = 0\n",
    "    while(n != len(sent)):\n",
    "        if n < len(sent) - 2 and (sent[n] + \"‌\" + sent[n+1] + \"‌\" + sent[n+2]) in embeddings_index:\n",
    "            tokenized.append(sent[n] + \"‌\" + sent[n+1] + \"‌\" + sent[n+2])\n",
    "            n += 2\n",
    "        elif n != len(sent) -1 and (sent[n] + \"‌\" + sent[n+1]) in embeddings_index:\n",
    "            tokenized.append(sent[n] + \"‌\" + sent[n+1])\n",
    "            n += 1\n",
    "        else:\n",
    "            tokenized.append(sent[n])\n",
    "        n += 1\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_size = 5\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for i in range(1,class_size+1):\n",
    "    with open('class{}.csv'.format(i)) as f:\n",
    "        reader = csv.reader(f)\n",
    "        class_list = [r[0] for r in reader]\n",
    "        y = [1 if j == i else 0 for j in range(1 , class_size+1)]\n",
    "        class_list = [(o,y) for o in class_list]\n",
    "        X_train = X_train + class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(X_train)\n",
    "shuffle(X_train)\n",
    "shuffle(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = [i[1] for i in X_train]\n",
    "X_train = [prepareSent(i[0]) for i in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ = 20\n",
    "def wordToVec(data):\n",
    "    for s in range(len(data)):\n",
    "        n = MAX_SEQ - len(data[s])\n",
    "        if n < 0:\n",
    "            data[s] = data[s][:MAX_SEQ]\n",
    "        else:\n",
    "            for i in range(n):\n",
    "                data[s].append('<PAD>')\n",
    "        for v in range(len(data[s])):\n",
    "            if data[s][v] not in embeddings_index:\n",
    "                data[s][v] = embeddings_index['<UNK>']\n",
    "            else:\n",
    "                data[s][v] = embeddings_index[data[s][v]]\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(503, 20, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = wordToVec(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Building model with Batch size 64</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 20, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 40)                51360     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 205       \n",
      "=================================================================\n",
      "Total params: 51,565\n",
      "Trainable params: 51,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "input_layer = Input( batch_shape = (None, MAX_SEQ, 300))\n",
    "lstm_layer = Bidirectional(LSTM(units=MAX_SEQ))(input_layer)\n",
    "output_layer = Dense(class_size, activation=\"softmax\")(lstm_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:500]\n",
    "Y_train = np.array(Y_train[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('weight_farsi_dekhte.{epoch:02d}.hdf5', period=25)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.5697\n",
      "Epoch 2/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.4266\n",
      "Epoch 3/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 1.1189\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.8214\n",
      "Epoch 5/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5814\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4248\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3061A: 1s \n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.2167\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1562\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1164\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.1089\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0881\n",
      "Epoch 13/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0611\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0506\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0438\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0385\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0340\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0301\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0253\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0231\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0214\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0194\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0186\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0175\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0144\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0137\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0124\n",
      "Epoch 28/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0126\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0103\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0094\n",
      "Epoch 31/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0089\n",
      "Epoch 32/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0084\n",
      "Epoch 33/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0079\n",
      "Epoch 34/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0076\n",
      "Epoch 35/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0072\n",
      "Epoch 36/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0064\n",
      "Epoch 37/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0064\n",
      "Epoch 38/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0077\n",
      "Epoch 39/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0062\n",
      "Epoch 40/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0053\n",
      "Epoch 41/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0049\n",
      "Epoch 42/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0045\n",
      "Epoch 43/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0044\n",
      "Epoch 44/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0044\n",
      "Epoch 45/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0039\n",
      "Epoch 46/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0310\n",
      "Epoch 47/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0321\n",
      "Epoch 48/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0118\n",
      "Epoch 49/50\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 0.0253\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f32a2f967b8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model\n",
    "EPOCH_SIZE = 50\n",
    "model.fit(X_train, Y_train, epochs=EPOCH_SIZE, batch_size=BATCH_SIZE, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining entities\n",
    "embeddings_size = 300\n",
    "entity_lists = { \"cloth\" : ['شلوار' , 'کاپشن' , 'پیراهن'],\n",
    "                \"city_iran\" : ['شیراز', 'کرج', 'مشهد'],\n",
    "                \"food\" : ['کباب', 'ساندویچ', 'سوپ'],\n",
    "                \"time\" : ['امروز', 'فردا', 'دیروز']\n",
    "               }\n",
    "for ent in entity_lists:\n",
    "    sum_of_embedding = np.zeros(embeddings_size)\n",
    "    for obj in entity_lists[ent]:\n",
    "        sum_of_embedding += np.array(embeddings_index[obj])\n",
    "    sum_of_embedding /= len(entity_lists[ent])\n",
    "    globals()['entity_{}'.format(ent)] = list(sum_of_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recognize entity\n",
    "def return_entity(sent , entity):\n",
    "    sent = sent.lower()\n",
    "    ma = 0\n",
    "    ans = \"\"\n",
    "    for i in sent.split():\n",
    "        if sim(embeddings_index[i] , entity) > ma:\n",
    "            ma = sim(embeddings_index[i] , entity)\n",
    "            ans = i\n",
    "    if ma < .3:\n",
    "        return \"nothing\"\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def address(sent):\n",
    "    sent = sent.replace(' ','+')\n",
    "    res = requests.get(\"https://www.google.com/maps/search/?api=1&query={}+اصفهان&hl=fa\".format(sent))\n",
    "    print(res.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather(sent):\n",
    "    city = return_entity(sent, entity_city_iran)\n",
    "    if city == 'nothing':\n",
    "        city = 'اصفهان'\n",
    "    blob = TextBlob(city)\n",
    "    blob = blob.translate(to=\"en\")\n",
    "    res = requests.get(\"https://api.openweathermap.org/data/2.5/weather?q={}&appid=3bb1d3931ea6593a0833bd5cf0b97ac3\".format(blob))\n",
    "    if res.status_code == 200:\n",
    "        data = json.loads(res.text)\n",
    "        temp = int(data['main']['temp'] - 273.15)\n",
    "        desc = data['weather'][0]['description']\n",
    "        blob = TextBlob(desc)\n",
    "        desc = blob.translate(to=\"fa\")\n",
    "        print('هوای  {} {}'.format(city, desc))\n",
    "        print('و دمای هوا {} درجه سانتیگراد می‌باشد'.format(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whichClass(inp):\n",
    "    for i in range(len(inp)):\n",
    "        if 1 == inp[i]:\n",
    "            if i == 0:\n",
    "                return \"Address\"\n",
    "            elif i == 1:\n",
    "                return \"Resturant\"\n",
    "            elif i == 2:\n",
    "                return \"Home Service\"\n",
    "            elif i == 3:\n",
    "                return \"Laundary\"\n",
    "            elif i == 4:\n",
    "                return \"Weather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(sent):\n",
    "    sentence = prepareSent(sent)\n",
    "    sent = ' '.join(list(sentence))\n",
    "    sentence = wordToVec([sentence])[0]\n",
    "    sentence = np.reshape(sentence , (1 , 20 , 300))\n",
    "    \n",
    "    sentence = model.predict(sentence)\n",
    "    argmax = np.argmax(sentence)\n",
    "    a = [1 if argmax == i else 0 for i in range(class_size)]\n",
    "    res = whichClass(list(a))\n",
    "    if res == 'Address':\n",
    "        address(sent)\n",
    "    elif res == 'Weather':\n",
    "        weather(sent)\n",
    "    else:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/maps/search/?api=1&query=%D8%B1%D8%B3%D8%AA%D9%88%D8%B1%D8%A7%D9%86+%D8%AE%D9%88%D8%A8+%D8%AA%D9%88+%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86+%D9%87%D8%B3%D8%AA%D8%9F+%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86&hl=fa\n"
     ]
    }
   ],
   "source": [
    "sent = \"میدونی چجوری باید برم سی و سه پل\"\n",
    "sent = 'رستوران خوب تو اصفهان هست؟'\n",
    "classify(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اصفهان\n",
      "هوای  اصفهان چند ابرها\n",
      "و دمای هوا 35 درجه سانتیگراد می‌باشد\n"
     ]
    }
   ],
   "source": [
    "sent = 'باران دوست داری ؟'\n",
    "# prepareSent('میاد؟')\n",
    "classify(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
