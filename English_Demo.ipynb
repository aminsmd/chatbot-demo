{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM,Dense,Input,Bidirectional\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "import pickle\n",
    "import gensim.models as gm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Preparing our Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dekhteX_train100', 'rb') as dekht:\n",
    "    X_train = pickle.load(dekht)\n",
    "with open('dekhteY_train100', 'rb') as dekhty:\n",
    "    Y_train = pickle.load(dekhty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 7)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#casting labels\n",
    "Y_train = np.array(Y_train)\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Reading our word embedding</h2>\n",
    "and preparing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open('/Users/mohammad/Documents/Internship-IAI/indian hotel/glove.6B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        \n",
    "        coefs = [float(i) for i in values[1:]]\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embeddings_index['<PAD>'] = [0] * 300\n",
    "embeddings_index['<UNK>'] = [1] * 300\n",
    "\n",
    "# word_embedding = gm.KeyedVectors.load_word2vec_format('/Users/mohammad/Documents/Internship-IAI/indian hotel/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# unk_index = [1] * 300\n",
    "# pad_index = [0] * 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: red\">proccessing sentences and replacing word vectors with words</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ = 20\n",
    "for s in range(len(X_train)):\n",
    "    n = MAX_SEQ - len(X_train[s])\n",
    "    if n < 0:\n",
    "        X_train[s] = X_train[s][:MAX_SEQ]\n",
    "    else:\n",
    "        for i in range(n):\n",
    "            X_train[s].append('<PAD>')\n",
    "    for v in range(len(X_train[s])):\n",
    "        try:\n",
    "            X_train[s][v] = list(word_embedding.word_vec(X_train[s][v]))\n",
    "            \n",
    "        except:\n",
    "            X_train[s][v] = unk_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 20, 300)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#casting training set\n",
    "X_train = np.array(X_train)\n",
    "X_train.shape\n",
    "# word_embedding.word_vec(X_train[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Building model with Batch size 64</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (64, 20, 300)             0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (64, 40)                  51360     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (64, 7)                   287       \n",
      "=================================================================\n",
      "Total params: 51,647\n",
      "Trainable params: 51,647\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "input_layer = Input( batch_shape = (BATCH_SIZE, MAX_SEQ, 300))\n",
    "lstm_layer = Bidirectional(LSTM(units=MAX_SEQ))(input_layer)\n",
    "output_layer = Dense(7, activation=\"softmax\")(lstm_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checkpoints at the end of each epoch\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('weight_dekhte.{epoch:02d}.hdf5')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #training resize\n",
    "# X_train = X_train[0:640]\n",
    "# Y_train = Y_train[0:640]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load previous weights\n",
    "# model.load_weights('weight_dekhte.60.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fit the model\n",
    "# EPOCH_SIZE = 60\n",
    "# model.fit(X_train, Y_train, epochs=EPOCH_SIZE, batch_size=BATCH_SIZE, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Building model with Batch size 1</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (1, 20, 300)              0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (1, 40)                   51360     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (1, 7)                    287       \n",
      "=================================================================\n",
      "Total params: 51,647\n",
      "Trainable params: 51,647\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "input_layer = Input( batch_shape = (BATCH_SIZE, MAX_SEQ, 300))\n",
    "lstm_layer = Bidirectional(LSTM(units=MAX_SEQ))(input_layer)\n",
    "output_layer = Dense(7, activation=\"softmax\")(lstm_layer)\n",
    "\n",
    "dekhtemodel = Model(inputs=input_layer, outputs=output_layer)\n",
    "dekhtemodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam')\n",
    "dekhtemodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer prev model weights\n",
    "# we = model.get_weights()\n",
    "# dekhtemodel.set_weights(we)\n",
    "dekhtemodel.load_weights('last_weights.hdf5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load previous weights\n",
    "# dekhtemodel.load_weights('weight_dekhte.40.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (1, 20, 300)              0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (1, 40)                   51360     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (1, 7)                    287       \n",
      "=================================================================\n",
      "Total params: 51,647\n",
      "Trainable params: 51,647\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dekhtemodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam')\n",
    "dekhtemodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #fit model\n",
    "# EPOCH_SIZE = 12\n",
    "# dekhtemodel.fit(X_train, Y_train, epochs=EPOCH_SIZE, batch_size=BATCH_SIZE, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Preparing Sentences for testing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(sentence):\n",
    "  tokenizer = TreebankWordTokenizer()\n",
    "  sent = tokenizer.tokenize(sentence)\n",
    "  for i in sent:\n",
    "    n = MAX_SEQ - len(sent)\n",
    "    if n < 0:\n",
    "      sent = sent[:MAX_SEQ]\n",
    "    else:\n",
    "        for j in range(n):\n",
    "            sent.append('<PAD>')\n",
    "  for j in range(len(sent)):\n",
    "    try:\n",
    "      sent[j] = list(word_embedding.word_vec(sent[j]))\n",
    "    except:\n",
    "      sent[j] = unk_index\n",
    "  return np.array(sent).reshape((1, 20, 300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(sent):\n",
    "    sentence = prepare(sent)\n",
    "    sentence = dekhtemodel.predict(sentence)\n",
    "    argmax = np.argmax(sentence)\n",
    "    if argmax == 0:\n",
    "        print('AddToPlaylist')\n",
    "    elif argmax == 1:\n",
    "        print('BookRestaurant')\n",
    "    elif argmax == 2:\n",
    "#         print('GetWeather')\n",
    "        city = return_entity(sent , entity_city_iran)\n",
    "        day = return_entity(sent, entity_day)\n",
    "        print('you requested ' + city +'\\'s weather for ' + day +\"?\")\n",
    "    \n",
    "    elif argmax == 3:\n",
    "        print('PlayMusic')\n",
    "    elif argmax == 4:\n",
    "        print('RateBook')\n",
    "    elif argmax == 5:\n",
    "        print('SearchCreativeWork')\n",
    "    elif argmax == 6:\n",
    "        print('SearchScreeningEvent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Entity recognitions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity function\n",
    "from scipy import spatial\n",
    "def sim(dataSetI , dataSetII):\n",
    "    return 1 - spatial.distance.cosine(dataSetI, dataSetII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recognize entity\n",
    "def return_entity(sent , entity):\n",
    "    sent = sent.lower()\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    sent = tokenizer.tokenize(sent)\n",
    "    ma = 0\n",
    "    ans = \"\"\n",
    "    for i in sent: \n",
    "        try:\n",
    "            if i not in stop_words and sim(list(word_embedding.word_vec(i)) , entity) > ma:\n",
    "                ma = sim(list(word_embedding.word_vec(i)) , entity)\n",
    "                ans = i\n",
    "        except:\n",
    "            pass\n",
    "    if ma < .1:\n",
    "        return \"nothing\"\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining entities\n",
    "from nltk.corpus import stopwords\n",
    "embeddings_size = 300\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('?')\n",
    "entity_lists = { \"cloth\" : ['t_shirt' , 'shirts' , 'jeans'],\n",
    "                \"city_iran\" : ['tehran', 'karaj', 'san_francisco'],\n",
    "                \"name_foreign\" : ['john', 'jack', 'paul'],\n",
    "                \"music_genre\" : ['pop', 'rap', 'jazz', 'rock', 'classical'],\n",
    "                \"day\" : ['tomorrow', 'today', 'yesterday', 'friday', 'sunday', 'saturdays'],\n",
    "                \"adverb\": ['sometimes', 'usually', 'never']\n",
    "                \n",
    "               }\n",
    "for ent in entity_lists:\n",
    "    sum_of_embedding = np.zeros(embeddings_size)\n",
    "    for obj in entity_lists[ent]:\n",
    "        sum_of_embedding += word_embedding.word_vec(obj)\n",
    "#         sum_of_embedding += np.array(embeddings_index[obj])\n",
    "    sum_of_embedding /= len(entity_lists[ent])\n",
    "    globals()['entity_{}'.format(ent)] = list(sum_of_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Test</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you requested sunday's weather for sunday?\n"
     ]
    }
   ],
   "source": [
    "sent1 = 'is it going to rain in texas often sunday with a R&B and some pants?'\n",
    "sent = 'is it cold tomorrow in tehran'\n",
    "sent = 'add song to playlist'\n",
    "sent = 'i want to hear something from micheal jackson'\n",
    "sent = \"how is the weather in newyork tomorrow\"\n",
    "classify(sent1)\n",
    "# print(\"city in iran: \" ,return_entity(sent , entity_city_iran))\n",
    "# print(\"adverb: \", return_entity(sent, entity_adverb))\n",
    "# print(\"time: \", return_entity(sent, entity_day))\n",
    "# print(\"genre: \", return_entity(sent, entity_music_genre))\n",
    "# print(\"clothes: \", return_entity(sent, entity_cloth))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
