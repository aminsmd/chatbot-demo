{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# essential imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (pywrap_tensorflow_internal.py, line 114)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2961\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-4-2e6b2a9c49e2>\"\u001b[0m, line \u001b[1;32m8\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from keras.preprocessing.text import Tokenizer\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.7/site-packages/keras/__init__.py\"\u001b[0m, line \u001b[1;32m3\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from . import utils\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.7/site-packages/keras/utils/__init__.py\"\u001b[0m, line \u001b[1;32m6\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from . import conv_utils\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.7/site-packages/keras/utils/conv_utils.py\"\u001b[0m, line \u001b[1;32m9\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from .. import backend as K\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.7/site-packages/keras/backend/__init__.py\"\u001b[0m, line \u001b[1;32m89\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from .tensorflow_backend import *\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\"\u001b[0m, line \u001b[1;32m5\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    import tensorflow as tf\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.7/site-packages/tensorflow/__init__.py\"\u001b[0m, line \u001b[1;32m24\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.7/site-packages/tensorflow/python/__init__.py\"\u001b[0m, line \u001b[1;32m49\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from tensorflow.python import pywrap_tensorflow\n",
      "\u001b[0;36m  File \u001b[0;32m\"/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\"\u001b[0;36m, line \u001b[0;32m58\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from tensorflow.python.pywrap_tensorflow_internal import *\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\"\u001b[0;36m, line \u001b[0;32m114\u001b[0m\n\u001b[0;31m    def TFE_ContextOptionsSetAsync(arg1, async):\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Packages for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Packages for modeling\n",
    "from keras import models\n",
    "from keras import Model\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional, Input, Dropout, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reading train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "j = 0\n",
    "with open(\"train.ft.txt\",\"r\") as f:\n",
    "    for i in f:\n",
    "        if j == 30000: #for computation reasons\n",
    "            break\n",
    "        data.append(i)\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initializing hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_WORDS = 50000  # Parameter indicating the number of words we'll put in the dictionary\n",
    "NB_START_EPOCHS = 26  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 512  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extracting labels from sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(data) :\n",
    "    labels = []\n",
    "    for i in data :\n",
    "        if(((i.split()[0]).replace(\"__label__\",\"\"))=='1'):\n",
    "            labels.append([1,0])\n",
    "        else :\n",
    "            labels.append([0,1])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# removing stopwords and \"__label__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(input_text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "    return \" \".join(clean_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_label(data):\n",
    "    d = []\n",
    "    for i in data :\n",
    "        d.append((i.replace(\"__label__\",\"\"))[2:].replace(\"\\n\",\"\"))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_label(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Worst!: A complete waste of time. Typographical errors, poor grammar, and a totally pathetic plot add up to absolutely nothing. I'm embarrassed for this author and very disappointed I actually paid for this book.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_sentences(data):    \n",
    "    for i in range(len(data)):\n",
    "        data[i] = remove_stopwords(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_sentences(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stuning even non-gamer: This sound track beautiful! It paints senery mind well would recomend even people hate vid. game music! played game Chrono Cross games ever played best music! It backs away crude keyboarding takes fresher step grate guitars soulful orchestras. It would impress anyone cares listen! ^_^'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reading word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(filename = 'glove.6B.300d.txt' ,dimension = 300)\n",
    "    embeddings_index = {}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "\n",
    "            coefs = [float(i) for i in values[1:]]\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embeddings_index['<PAD>'] = [0] * dimension\n",
    "    embeddings_index['<UNK>'] = [1] * dimension\n",
    "    return embeddings_index\n",
    "    \n",
    "embeddings_index = read_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizing and removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(data):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    training = []\n",
    "    for i in range(len(data)):\n",
    "        training.append(tokenizer.tokenize(data[i]))\n",
    "    for i in range(len(training)):\n",
    "        training[i] = [x.lower() for x in training[i]]\n",
    "    return training\n",
    "\n",
    "training = tokenizing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct(training)\n",
    "    punct = ['!','\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', \n",
    "             '[', '/', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n']\n",
    "    train_X =[]\n",
    "    for i in range(len(training)):\n",
    "        sentence = []\n",
    "        for j in range(len(training[i])):\n",
    "            if training[i][j] in punct:\n",
    "                pass\n",
    "            else:\n",
    "                sentence.append(training[i][j])\n",
    "        train_X.append(sentence)\n",
    "    return train_X\n",
    "\n",
    "train_X = punct(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting words to embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2embed(train_X , MAX_SEQ = 120)\n",
    "    #MAX_SEQ = 120\n",
    "    for s in range(len(train_X)):\n",
    "        n = MAX_SEQ - len(train_X[s])\n",
    "        if n < 0:\n",
    "            train_X[s] = train_X[s][:MAX_SEQ]\n",
    "        else:\n",
    "            for i in range(n):\n",
    "                train_X[s].append('<PAD>')\n",
    "        for v in range(len(train_X[s])):\n",
    "            if train_X[s][v] not in embeddings_index:\n",
    "                train_X[s][v] = embeddings_index['<UNK>']\n",
    "            else:\n",
    "                train_X[s][v] = embeddings_index[train_X[s][v]]\n",
    "word2embed(train_X , 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X = train_X[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 120, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = np.array(train_X)\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 120, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 240)               404160    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 240)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                4820      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 409,102\n",
      "Trainable params: 409,062\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ = 120\n",
    "input_layer = Input( batch_shape = (None, MAX_SEQ, 300))\n",
    "lstm_layer = Bidirectional(LSTM(units=MAX_SEQ, dropout = 0.25, recurrent_dropout=0.25))(input_layer)\n",
    "x = Dropout(0.25)(lstm_layer)\n",
    "merged = Dense(units=20, activation='relu')(x)\n",
    "merged = Dropout(0.25)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "output_layer = Dense(2, activation=\"softmax\")(merged)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam'\n",
    "              , loss='binary_crossentropy'\n",
    "              , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adding checkpoint for weights and setting validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('weight_sentiment_amazon.{epoch:02d}.hdf5')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = train_X[:BATCH_SIZE]\n",
    "Y_valid = labels[:BATCH_SIZE]\n",
    "train_X = train_X[BATCH_SIZE:]\n",
    "y_train_oh = labels[BATCH_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(train_X)\n",
    "X_valid = np.array(X_valid)\n",
    "Y_valid = np.array(Y_valid)\n",
    "y_train_oh = np.array(y_train_oh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[0]\n",
    "#X_valid.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29488 samples, validate on 512 samples\n",
      "Epoch 1/26\n",
      "29488/29488 [==============================] - 403s 14ms/step - loss: 0.5991 - acc: 0.7120 - val_loss: 0.3895 - val_acc: 0.8281\n",
      "Epoch 2/26\n",
      "29488/29488 [==============================] - 386s 13ms/step - loss: 0.4618 - acc: 0.7875 - val_loss: 0.3549 - val_acc: 0.8496\n",
      "Epoch 3/26\n",
      "29488/29488 [==============================] - 369s 13ms/step - loss: 0.4207 - acc: 0.8102 - val_loss: 0.3486 - val_acc: 0.8594\n",
      "Epoch 4/26\n",
      "29488/29488 [==============================] - 368s 12ms/step - loss: 0.4024 - acc: 0.8216 - val_loss: 0.4006 - val_acc: 0.8340\n",
      "Epoch 5/26\n",
      "29488/29488 [==============================] - 368s 12ms/step - loss: 0.3806 - acc: 0.8328 - val_loss: 0.3140 - val_acc: 0.8691\n",
      "Epoch 6/26\n",
      "29488/29488 [==============================] - 368s 12ms/step - loss: 0.3665 - acc: 0.8409 - val_loss: 0.3674 - val_acc: 0.8477\n",
      "Epoch 7/26\n",
      "29488/29488 [==============================] - 361s 12ms/step - loss: 0.3478 - acc: 0.8518 - val_loss: 0.3059 - val_acc: 0.8691\n",
      "Epoch 8/26\n",
      "29488/29488 [==============================] - 364s 12ms/step - loss: 0.3322 - acc: 0.8579 - val_loss: 0.2891 - val_acc: 0.8809\n",
      "Epoch 9/26\n",
      "29488/29488 [==============================] - 384s 13ms/step - loss: 0.3177 - acc: 0.8651 - val_loss: 0.3009 - val_acc: 0.8789\n",
      "Epoch 10/26\n",
      "29488/29488 [==============================] - 369s 13ms/step - loss: 0.3109 - acc: 0.8686 - val_loss: 0.2861 - val_acc: 0.8789\n",
      "Epoch 11/26\n",
      "29488/29488 [==============================] - 366s 12ms/step - loss: 0.2981 - acc: 0.8751 - val_loss: 0.2827 - val_acc: 0.8848\n",
      "Epoch 12/26\n",
      "29488/29488 [==============================] - 390s 13ms/step - loss: 0.2901 - acc: 0.8782 - val_loss: 0.3490 - val_acc: 0.8555\n",
      "Epoch 13/26\n",
      "29488/29488 [==============================] - 841s 29ms/step - loss: 0.2807 - acc: 0.8828 - val_loss: 0.2729 - val_acc: 0.8926\n",
      "Epoch 14/26\n",
      "29488/29488 [==============================] - 363s 12ms/step - loss: 0.2771 - acc: 0.8866 - val_loss: 0.2719 - val_acc: 0.8926\n",
      "Epoch 15/26\n",
      "29488/29488 [==============================] - 365s 12ms/step - loss: 0.2636 - acc: 0.8908 - val_loss: 0.2761 - val_acc: 0.9004\n",
      "Epoch 16/26\n",
      "29488/29488 [==============================] - 373s 13ms/step - loss: 0.2588 - acc: 0.8924 - val_loss: 0.2748 - val_acc: 0.8965\n",
      "Epoch 17/26\n",
      "29488/29488 [==============================] - 371s 13ms/step - loss: 0.2472 - acc: 0.8986 - val_loss: 0.2671 - val_acc: 0.8945\n",
      "Epoch 18/26\n",
      "29488/29488 [==============================] - 364s 12ms/step - loss: 0.2445 - acc: 0.8986 - val_loss: 0.2625 - val_acc: 0.8926\n",
      "Epoch 19/26\n",
      "29488/29488 [==============================] - 365s 12ms/step - loss: 0.2380 - acc: 0.9030 - val_loss: 0.2690 - val_acc: 0.8965\n",
      "Epoch 20/26\n",
      "29488/29488 [==============================] - 360s 12ms/step - loss: 0.2276 - acc: 0.9067 - val_loss: 0.2846 - val_acc: 0.8906\n",
      "Epoch 21/26\n",
      "29488/29488 [==============================] - 366s 12ms/step - loss: 0.2193 - acc: 0.9116 - val_loss: 0.3082 - val_acc: 0.8906\n",
      "Epoch 22/26\n",
      "29488/29488 [==============================] - 369s 13ms/step - loss: 0.2147 - acc: 0.9137 - val_loss: 0.2796 - val_acc: 0.8984\n",
      "Epoch 23/26\n",
      "29488/29488 [==============================] - 364s 12ms/step - loss: 0.2101 - acc: 0.9137 - val_loss: 0.2761 - val_acc: 0.9062\n",
      "Epoch 24/26\n",
      "29488/29488 [==============================] - 366s 12ms/step - loss: 0.2046 - acc: 0.9191 - val_loss: 0.2789 - val_acc: 0.8906\n",
      "Epoch 25/26\n",
      "29488/29488 [==============================] - 363s 12ms/step - loss: 0.1963 - acc: 0.9221 - val_loss: 0.2825 - val_acc: 0.8906\n",
      "Epoch 26/26\n",
      "29488/29488 [==============================] - 365s 12ms/step - loss: 0.1952 - acc: 0.9219 - val_loss: 0.2866 - val_acc: 0.8848\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_X, y_train_oh, validation_data=(X_valid, Y_valid), callbacks=callbacks_list, epochs=NB_START_EPOCHS\n",
    "                       , batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90625"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading final weights on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weight_sentiment_amazon.26.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting sentences to embedding vectors for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(inp):\n",
    "    punct = ['!','\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', \n",
    "         '[', '/', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n']\n",
    "    #neg = negate_sequence(inp)\n",
    "    for i in punct:\n",
    "        inp.replace(i, '')\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    inp = tokenizer.tokenize(inp)\n",
    "    MAX_SEQ = 120\n",
    "    n = MAX_SEQ - len(inp)\n",
    "    if n < 0:\n",
    "        inp = inp[:MAX_SEQ]\n",
    "    else:\n",
    "        for i in range(n):\n",
    "            inp.append('<PAD>')\n",
    "    for v in range(len(inp)):\n",
    "        if inp[v] not in embeddings_index:\n",
    "            inp[v] = embeddings_index['<UNK>']\n",
    "        else:\n",
    "            inp[v] = embeddings_index[inp[v]]\n",
    "    return np.reshape(np.array(inp) , (1, MAX_SEQ, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"the food was cold but I liked the taste.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41501242,  0.58498764]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(prep_data(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = []\n",
    "with open(\"test.ft.txt\",\"r\") as f:\n",
    "    for i in f:\n",
    "        data_test.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test set preprocess and labels extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = get_labels(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = remove_label(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_sentences(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = np.array(data_test)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = TreebankWordTokenizer()\n",
    "# testing = []\n",
    "# for i in range(len(data_test)):\n",
    "#     testing.append(tokenizer.tokenize(data_test[i]))\n",
    "# for i in range(len(testing)):\n",
    "#     testing[i] = [x.lower() for x in testing[i]]\n",
    "\n",
    "testing = tokenizing(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punct = ['!','\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', \n",
    "#          '[', '/', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n']\n",
    "# test_X =[]\n",
    "# for i in range(len(testing)):\n",
    "#     sentence = []\n",
    "#     for j in range(len(testing[i])):\n",
    "#         if testing[i][j] in punct:\n",
    "#             pass\n",
    "#         else:\n",
    "#             sentence.append(testing[i][j])\n",
    "#     test_X.append(sentence)\n",
    "    \n",
    "test_X = punct(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_X = test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_X[:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_SEQ = 120\n",
    "# for s in range(len(test_X)):\n",
    "#     n = MAX_SEQ - len(test_X[s])\n",
    "#     if n < 0:\n",
    "#         test_X[s] = test_X[s][:MAX_SEQ]\n",
    "#     else:\n",
    "#         for i in range(n):\n",
    "#             test_X[s].append('<PAD>')\n",
    "#     for v in range(len(test_X[s])):\n",
    "#         if test_X[s][v] not in embeddings_index:\n",
    "#             test_X[s][v] = embeddings_index['<UNK>']\n",
    "#         else:\n",
    "#             test_X[s][v] = embeddings_index[test_X[s][v]]\n",
    "            \n",
    "word2embed(test_X , 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluating the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 44s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29108472535014152, 0.89012500000000006]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(np.array(test_X),np.array(test_labels[:8000]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
