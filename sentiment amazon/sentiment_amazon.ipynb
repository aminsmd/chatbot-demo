{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Packages for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Packages for modeling\n",
    "from keras import models\n",
    "from keras import Model\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional, Input, Dropout, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "j = 0\n",
    "with open(\"train.ft.txt\",\"r\") as f:\n",
    "    for i in f:\n",
    "        if j == 30000:\n",
    "            break\n",
    "        data.append(i)\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_WORDS = 50000  # Parameter indicating the number of words we'll put in the dictionary\n",
    "NB_START_EPOCHS = 26  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 512  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(data) :\n",
    "    labels = []\n",
    "    for i in data :\n",
    "        if(((i.split()[0]).replace(\"__label__\",\"\"))=='1'):\n",
    "            labels.append([1,0])\n",
    "        else :\n",
    "            labels.append([0,1])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarytoneutral(data) :\n",
    "    labels = []\n",
    "    for i in data:\n",
    "        if list(i)==[1,0]:\n",
    "            labels.append([1,0,0])\n",
    "        else :\n",
    "            labels.append([0,0,1])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(input_text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "    return \" \".join(clean_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_label(data):\n",
    "    d = []\n",
    "    for i in data :\n",
    "        d.append((i.replace(\"__label__\",\"\"))[2:].replace(\"\\n\",\"\"))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_label(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Worst!: A complete waste of time. Typographical errors, poor grammar, and a totally pathetic plot add up to absolutely nothing. I'm embarrassed for this author and very disappointed I actually paid for this book.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_sentences(data):    \n",
    "    for i in range(len(data)):\n",
    "        data[i] = remove_stopwords(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_sentences(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stuning even non-gamer: This sound track beautiful! It paints senery mind well would recomend even people hate vid. game music! played game Chrono Cross games ever played best music! It backs away crude keyboarding takes fresher step grate guitars soulful orchestras. It would impress anyone cares listen! ^_^'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove.6B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        \n",
    "        coefs = [float(i) for i in values[1:]]\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embeddings_index['<PAD>'] = [0] * 300\n",
    "embeddings_index['<UNK>'] = [1] * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "training = []\n",
    "for i in range(len(data)):\n",
    "    training.append(tokenizer.tokenize(data[i]))\n",
    "for i in range(len(training)):\n",
    "    training[i] = [x.lower() for x in training[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = ['!','\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', \n",
    "         '[', '/', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n']\n",
    "train_X =[]\n",
    "for i in range(len(training)):\n",
    "    sentence = []\n",
    "    for j in range(len(training[i])):\n",
    "        if training[i][j] in punct:\n",
    "            pass\n",
    "        else:\n",
    "            sentence.append(training[i][j])\n",
    "    train_X.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ = 120\n",
    "for s in range(len(train_X)):\n",
    "    n = MAX_SEQ - len(train_X[s])\n",
    "    if n < 0:\n",
    "        train_X[s] = train_X[s][:MAX_SEQ]\n",
    "    else:\n",
    "        for i in range(n):\n",
    "            train_X[s].append('<PAD>')\n",
    "    for v in range(len(train_X[s])):\n",
    "        if train_X[s][v] not in embeddings_index:\n",
    "            train_X[s][v] = embeddings_index['<UNK>']\n",
    "        else:\n",
    "            train_X[s][v] = embeddings_index[train_X[s][v]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X = train_X[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 120, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = np.array(train_X)\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(NB_WORDS+3, embedding_size, input_length=MAX_SEQ))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.8)))\n",
    "# model.add(Dense(20, kernel_regularizer=regularizers.l1_l2(0.01)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer='adam'\n",
    "              , loss='binary_crossentropy'\n",
    "              , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 120, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 240)               404160    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 240)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                4820      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 409,102\n",
      "Trainable params: 409,062\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ = 120\n",
    "input_layer = Input( batch_shape = (None, MAX_SEQ, 300))\n",
    "lstm_layer = Bidirectional(LSTM(units=MAX_SEQ, dropout = 0.25, recurrent_dropout=0.25))(input_layer)\n",
    "x = Dropout(0.25)(lstm_layer)\n",
    "merged = Dense(units=20, activation='relu')(x)\n",
    "merged = Dropout(0.25)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "output_layer = Dense(2, activation=\"softmax\")(merged)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam'\n",
    "              , loss='binary_crossentropy'\n",
    "              , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('weight_sentiment_amazon.{epoch:02d}.hdf5')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = train_X[:BATCH_SIZE]\n",
    "Y_valid = labels[:BATCH_SIZE]\n",
    "train_X = train_X[BATCH_SIZE:]\n",
    "y_train_oh = labels[BATCH_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(train_X)\n",
    "X_valid = np.array(X_valid)\n",
    "Y_valid = np.array(Y_valid)\n",
    "y_train_oh = np.array(y_train_oh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[0]\n",
    "#X_valid.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29488 samples, validate on 512 samples\n",
      "Epoch 1/26\n",
      "29488/29488 [==============================] - 403s 14ms/step - loss: 0.5991 - acc: 0.7120 - val_loss: 0.3895 - val_acc: 0.8281\n",
      "Epoch 2/26\n",
      "29488/29488 [==============================] - 386s 13ms/step - loss: 0.4618 - acc: 0.7875 - val_loss: 0.3549 - val_acc: 0.8496\n",
      "Epoch 3/26\n",
      "29488/29488 [==============================] - 369s 13ms/step - loss: 0.4207 - acc: 0.8102 - val_loss: 0.3486 - val_acc: 0.8594\n",
      "Epoch 4/26\n",
      "29488/29488 [==============================] - 368s 12ms/step - loss: 0.4024 - acc: 0.8216 - val_loss: 0.4006 - val_acc: 0.8340\n",
      "Epoch 5/26\n",
      "29488/29488 [==============================] - 368s 12ms/step - loss: 0.3806 - acc: 0.8328 - val_loss: 0.3140 - val_acc: 0.8691\n",
      "Epoch 6/26\n",
      "29488/29488 [==============================] - 368s 12ms/step - loss: 0.3665 - acc: 0.8409 - val_loss: 0.3674 - val_acc: 0.8477\n",
      "Epoch 7/26\n",
      "29488/29488 [==============================] - 361s 12ms/step - loss: 0.3478 - acc: 0.8518 - val_loss: 0.3059 - val_acc: 0.8691\n",
      "Epoch 8/26\n",
      "29488/29488 [==============================] - 364s 12ms/step - loss: 0.3322 - acc: 0.8579 - val_loss: 0.2891 - val_acc: 0.8809\n",
      "Epoch 9/26\n",
      "29488/29488 [==============================] - 384s 13ms/step - loss: 0.3177 - acc: 0.8651 - val_loss: 0.3009 - val_acc: 0.8789\n",
      "Epoch 10/26\n",
      "29488/29488 [==============================] - 369s 13ms/step - loss: 0.3109 - acc: 0.8686 - val_loss: 0.2861 - val_acc: 0.8789\n",
      "Epoch 11/26\n",
      "29488/29488 [==============================] - 366s 12ms/step - loss: 0.2981 - acc: 0.8751 - val_loss: 0.2827 - val_acc: 0.8848\n",
      "Epoch 12/26\n",
      "29488/29488 [==============================] - 390s 13ms/step - loss: 0.2901 - acc: 0.8782 - val_loss: 0.3490 - val_acc: 0.8555\n",
      "Epoch 13/26\n",
      "29488/29488 [==============================] - 841s 29ms/step - loss: 0.2807 - acc: 0.8828 - val_loss: 0.2729 - val_acc: 0.8926\n",
      "Epoch 14/26\n",
      "29488/29488 [==============================] - 363s 12ms/step - loss: 0.2771 - acc: 0.8866 - val_loss: 0.2719 - val_acc: 0.8926\n",
      "Epoch 15/26\n",
      "29488/29488 [==============================] - 365s 12ms/step - loss: 0.2636 - acc: 0.8908 - val_loss: 0.2761 - val_acc: 0.9004\n",
      "Epoch 16/26\n",
      "29488/29488 [==============================] - 373s 13ms/step - loss: 0.2588 - acc: 0.8924 - val_loss: 0.2748 - val_acc: 0.8965\n",
      "Epoch 17/26\n",
      "29488/29488 [==============================] - 371s 13ms/step - loss: 0.2472 - acc: 0.8986 - val_loss: 0.2671 - val_acc: 0.8945\n",
      "Epoch 18/26\n",
      "29488/29488 [==============================] - 364s 12ms/step - loss: 0.2445 - acc: 0.8986 - val_loss: 0.2625 - val_acc: 0.8926\n",
      "Epoch 19/26\n",
      "29488/29488 [==============================] - 365s 12ms/step - loss: 0.2380 - acc: 0.9030 - val_loss: 0.2690 - val_acc: 0.8965\n",
      "Epoch 20/26\n",
      "29488/29488 [==============================] - 360s 12ms/step - loss: 0.2276 - acc: 0.9067 - val_loss: 0.2846 - val_acc: 0.8906\n",
      "Epoch 21/26\n",
      "29488/29488 [==============================] - 366s 12ms/step - loss: 0.2193 - acc: 0.9116 - val_loss: 0.3082 - val_acc: 0.8906\n",
      "Epoch 22/26\n",
      "29488/29488 [==============================] - 369s 13ms/step - loss: 0.2147 - acc: 0.9137 - val_loss: 0.2796 - val_acc: 0.8984\n",
      "Epoch 23/26\n",
      "29488/29488 [==============================] - 364s 12ms/step - loss: 0.2101 - acc: 0.9137 - val_loss: 0.2761 - val_acc: 0.9062\n",
      "Epoch 24/26\n",
      "29488/29488 [==============================] - 366s 12ms/step - loss: 0.2046 - acc: 0.9191 - val_loss: 0.2789 - val_acc: 0.8906\n",
      "Epoch 25/26\n",
      "29488/29488 [==============================] - 363s 12ms/step - loss: 0.1963 - acc: 0.9221 - val_loss: 0.2825 - val_acc: 0.8906\n",
      "Epoch 26/26\n",
      "29488/29488 [==============================] - 365s 12ms/step - loss: 0.1952 - acc: 0.9219 - val_loss: 0.2866 - val_acc: 0.8848\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_X, y_train_oh, validation_data=(X_valid, Y_valid), callbacks=callbacks_list, epochs=NB_START_EPOCHS\n",
    "                       , batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90625"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weight_sentiment_amazon.26.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(inp):\n",
    "    punct = ['!','\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', \n",
    "         '[', '/', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n']\n",
    "    #neg = negate_sequence(inp)\n",
    "    for i in punct:\n",
    "        inp.replace(i, '')\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    inp = tokenizer.tokenize(inp)\n",
    "    MAX_SEQ = 120\n",
    "    n = MAX_SEQ - len(inp)\n",
    "    if n < 0:\n",
    "        inp = inp[:MAX_SEQ]\n",
    "    else:\n",
    "        for i in range(n):\n",
    "            inp.append('<PAD>')\n",
    "    for v in range(len(inp)):\n",
    "        if inp[v] not in embeddings_index:\n",
    "            inp[v] = embeddings_index['<UNK>']\n",
    "        else:\n",
    "            inp[v] = embeddings_index[inp[v]]\n",
    "    return np.reshape(np.array(inp) , (1, MAX_SEQ, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"the food was cold but I liked the taste.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41501242,  0.58498764]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(prep_data(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = []\n",
    "with open(\"test.ft.txt\",\"r\") as f:\n",
    "    for i in f:\n",
    "        data_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = get_labels(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = remove_label(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_sentences(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = np.array(data_test)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "testing = []\n",
    "for i in range(len(data_test)):\n",
    "    testing.append(tokenizer.tokenize(data_test[i]))\n",
    "for i in range(len(testing)):\n",
    "    testing[i] = [x.lower() for x in testing[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = ['!','\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', \n",
    "         '[', '/', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n']\n",
    "test_X =[]\n",
    "for i in range(len(testing)):\n",
    "    sentence = []\n",
    "    for j in range(len(testing[i])):\n",
    "        if testing[i][j] in punct:\n",
    "            pass\n",
    "        else:\n",
    "            sentence.append(testing[i][j])\n",
    "    test_X.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_X = test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_X[:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ = 120\n",
    "for s in range(len(test_X)):\n",
    "    n = MAX_SEQ - len(test_X[s])\n",
    "    if n < 0:\n",
    "        test_X[s] = test_X[s][:MAX_SEQ]\n",
    "    else:\n",
    "        for i in range(n):\n",
    "            test_X[s].append('<PAD>')\n",
    "    for v in range(len(test_X[s])):\n",
    "        if test_X[s][v] not in embeddings_index:\n",
    "            test_X[s][v] = embeddings_index['<UNK>']\n",
    "        else:\n",
    "            test_X[s][v] = embeddings_index[test_X[s][v]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 44s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29108472535014152, 0.89012500000000006]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(np.array(test_X),np.array(test_labels[:8000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing twitter model on Amazon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 20, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 40)                51360     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 52,323\n",
      "Trainable params: 52,283\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ = 20\n",
    "input_layer = Input( batch_shape = (None, MAX_SEQ, 300))\n",
    "lstm_layer = Bidirectional(LSTM(units=MAX_SEQ, dropout = 0.25, recurrent_dropout=0.25))(input_layer)\n",
    "x = Dropout(0.25)(lstm_layer)\n",
    "merged = Dense(units=20, activation='relu')(x)\n",
    "merged = Dropout(0.25)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "output_layer = Dense(3, activation=\"softmax\")(merged)\n",
    "\n",
    "model_twitter = Model(inputs=input_layer, outputs=output_layer)\n",
    "model_twitter.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_twitter.compile(optimizer='adam'\n",
    "              , loss='categorical_crossentropy'\n",
    "              , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_twitter.load_weights(\"weight_twitter_embedding.32.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_twitter = np.array(binarytoneutral(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_twitter = []\n",
    "for i in test_X:\n",
    "    test_twitter.append(i[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 5s 579us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2336834318637848, 0.65800000000000003]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_twitter.evaluate(np.array(test_twitter),np.array(labels_twitter[:8000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
