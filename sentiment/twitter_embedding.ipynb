{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Packages for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Packages for modeling\n",
    "from keras import models, Model\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional, Input\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "#from keras import initializations\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_WORDS = 50000  # Parameter indicating the number of words we'll put in the dictionary\n",
    "NB_START_EPOCHS = 32  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 128  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>@united 1k and had problem getting out of FLL ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>@virginamerica may start service to Hawaii fro...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>@united LHR arrival lounge #fail. Waited 20 mi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12694</th>\n",
       "      <td>@AmericanAir of delays and trapped on planes w...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12918</th>\n",
       "      <td>@AmericanAir you should really explain custome...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text airline_sentiment\n",
       "668    @united 1k and had problem getting out of FLL ...          negative\n",
       "319    @virginamerica may start service to Hawaii fro...           neutral\n",
       "718    @united LHR arrival lounge #fail. Waited 20 mi...          negative\n",
       "12694  @AmericanAir of delays and trapped on planes w...          negative\n",
       "12918  @AmericanAir you should really explain custome...          negative"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Tweets.csv')\n",
    "df = df.reindex(np.random.permutation(df.index))  \n",
    "df = df[['text', 'airline_sentiment']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>1k problem getting FLL IAH sent DM making con...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>may start service Hawaii #SanFrancisco year h...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>LHR arrival lounge #fail. Waited 20 mins show...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12694</th>\n",
       "      <td>delays trapped planes no water air. Never eve...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12918</th>\n",
       "      <td>really explain customer service gate agent 11...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text airline_sentiment\n",
       "668     1k problem getting FLL IAH sent DM making con...          negative\n",
       "319     may start service Hawaii #SanFrancisco year h...           neutral\n",
       "718     LHR arrival lounge #fail. Waited 20 mins show...          negative\n",
       "12694   delays trapped planes no water air. Never eve...          negative\n",
       "12918   really explain customer service gate agent 11...          negative"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text = df.text.apply(remove_stopwords).apply(remove_mentions)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train data samples: 13176\n",
      "# Test data samples: 1464\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.text, df.airline_sentiment, test_size=0.1, random_state=37)\n",
    "print('# Train data samples:', X_train.shape[0])\n",
    "print('# Test data samples:', X_test.shape[0])\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(num_words=NB_WORDS,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True,\n",
    "               split=\" \", \n",
    "               char_level=False)\n",
    "tk.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "training = []\n",
    "for i in range(len(X_train)):\n",
    "    training.append(tokenizer.tokenize(X_train[i]))\n",
    "for i in range(len(training)):\n",
    "    training[i] = [x.lower() for x in training[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "test = []\n",
    "for i in range(len(X_test)):\n",
    "    test.append(tokenizer.tokenize(X_test[i]))\n",
    "for i in range(len(test)):\n",
    "    test[i] = [x.lower() for x in test[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove.6B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        \n",
    "        coefs = [float(i) for i in values[1:]]\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embeddings_index['<PAD>'] = [0] * 300\n",
    "embeddings_index['<UNK>'] = [1] * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = ['!','\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', \n",
    "         '[', '/', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n']\n",
    "train_X =[]\n",
    "for i in range(len(training)):\n",
    "    sentence = []\n",
    "    for j in range(len(training[i])):\n",
    "        if training[i][j] in punct:\n",
    "            pass\n",
    "        else:\n",
    "            sentence.append(training[i][j])\n",
    "    train_X.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X =[]\n",
    "for i in range(len(test)):\n",
    "    sentence = []\n",
    "    for j in range(len(test[i])):\n",
    "        if test[i][j] in punct:\n",
    "            pass\n",
    "        else:\n",
    "            sentence.append(test[i][j])\n",
    "    test_X.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ = 20\n",
    "for s in range(len(train_X)):\n",
    "    n = MAX_SEQ - len(train_X[s])\n",
    "    if n < 0:\n",
    "        train_X[s] = train_X[s][:MAX_SEQ]\n",
    "    else:\n",
    "        for i in range(n):\n",
    "            train_X[s].append('<PAD>')\n",
    "    for v in range(len(train_X[s])):\n",
    "        if train_X[s][v] not in embeddings_index:\n",
    "            train_X[s][v] = embeddings_index['<UNK>']\n",
    "        else:\n",
    "            train_X[s][v] = embeddings_index[train_X[s][v]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ = 20\n",
    "for s in range(len(test_X)):\n",
    "    n = MAX_SEQ - len(test_X[s])\n",
    "    if n < 0:\n",
    "        test_X[s] = test_X[s][:MAX_SEQ]\n",
    "    else:\n",
    "        for i in range(n):\n",
    "            test_X[s].append('<PAD>')\n",
    "    for v in range(len(test_X[s])):\n",
    "        if test_X[s][v] not in embeddings_index:\n",
    "            test_X[s][v] = embeddings_index['<UNK>']\n",
    "        else:\n",
    "            test_X[s][v] = embeddings_index[test_X[s][v]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tk.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I've line half hour trying see representative, might even miss next flight too, unacceptable\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1520]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13176, 20, 300)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = np.array(train_X)\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1464, 20, 300)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = np.array(test_X)\n",
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"negative\" is converted into 1\n",
      "\"1\" is converted into [0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_le = le.fit_transform(y_train)\n",
    "y_test_le = le.transform(y_test)\n",
    "y_train_oh = to_categorical(y_train_le)\n",
    "y_test_oh = to_categorical(y_test_le)\n",
    "\n",
    "print('\"{}\" is converted into {}'.format(y_train[2], y_train_le[1]))\n",
    "print('\"{}\" is converted into {}'.format(y_train_le[1], y_train_oh[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 20, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 40)                51360     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 52,323\n",
      "Trainable params: 52,283\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input( batch_shape = (None, MAX_SEQ, 300))\n",
    "lstm_layer = Bidirectional(LSTM(units=MAX_SEQ, dropout = 0.25, recurrent_dropout=0.25))(input_layer)\n",
    "x = Dropout(0.25)(lstm_layer)\n",
    "merged = Dense(units=20, activation='relu')(x)\n",
    "merged = Dropout(0.25)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "output_layer = Dense(3, activation=\"softmax\")(merged)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam'\n",
    "              , loss='categorical_crossentropy'\n",
    "              , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('weight_twitter_embedding.32.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('weight_twitter_embedding.{epoch:02d}.hdf5', monitor='val_loss', save_best_only=True)\n",
    "tb = TensorBoard(log_dir='./Graph', histogram_freq=0,  \n",
    "          write_graph=True, write_images=True)\n",
    "es = EarlyStopping(monitor='val_loss', \n",
    "                   min_delta=0.01,\n",
    "                   patience=3,\n",
    "                   verbose=0,\n",
    "                   mode='auto')\n",
    "callbacks_list = [checkpoint, tb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = train_X[:10*BATCH_SIZE]\n",
    "Y_valid = y_train_oh[:10*BATCH_SIZE]\n",
    "train_X = train_X[10*BATCH_SIZE:]\n",
    "y_train_oh = y_train_oh[10*BATCH_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11896 samples, validate on 1280 samples\n",
      "Epoch 1/32\n",
      "11896/11896 [==============================] - 8s 650us/step - loss: 0.4820 - acc: 0.8112 - val_loss: 0.4989 - val_acc: 0.8008\n",
      "Epoch 2/32\n",
      "11896/11896 [==============================] - 8s 646us/step - loss: 0.4693 - acc: 0.8181 - val_loss: 0.4997 - val_acc: 0.8055\n",
      "Epoch 3/32\n",
      "11896/11896 [==============================] - 8s 638us/step - loss: 0.4715 - acc: 0.8190 - val_loss: 0.4992 - val_acc: 0.8031\n",
      "Epoch 4/32\n",
      "11896/11896 [==============================] - 8s 649us/step - loss: 0.4588 - acc: 0.8233 - val_loss: 0.4970 - val_acc: 0.8039\n",
      "Epoch 5/32\n",
      "11896/11896 [==============================] - 8s 641us/step - loss: 0.4533 - acc: 0.8264 - val_loss: 0.4938 - val_acc: 0.8070\n",
      "Epoch 6/32\n",
      "11896/11896 [==============================] - 8s 648us/step - loss: 0.4498 - acc: 0.8273 - val_loss: 0.5005 - val_acc: 0.8047\n",
      "Epoch 7/32\n",
      "11896/11896 [==============================] - 8s 648us/step - loss: 0.4414 - acc: 0.8291 - val_loss: 0.4922 - val_acc: 0.8078\n",
      "Epoch 8/32\n",
      "11896/11896 [==============================] - 8s 659us/step - loss: 0.4294 - acc: 0.8341 - val_loss: 0.5054 - val_acc: 0.8008\n",
      "Epoch 9/32\n",
      "11896/11896 [==============================] - 8s 653us/step - loss: 0.4313 - acc: 0.8322 - val_loss: 0.5076 - val_acc: 0.7992\n",
      "Epoch 10/32\n",
      "11896/11896 [==============================] - 8s 666us/step - loss: 0.4269 - acc: 0.8339 - val_loss: 0.5140 - val_acc: 0.7961\n",
      "Epoch 11/32\n",
      "11896/11896 [==============================] - 8s 674us/step - loss: 0.4199 - acc: 0.8369 - val_loss: 0.5158 - val_acc: 0.7953\n",
      "Epoch 12/32\n",
      "11896/11896 [==============================] - 8s 667us/step - loss: 0.4141 - acc: 0.8437 - val_loss: 0.5045 - val_acc: 0.7984\n",
      "Epoch 13/32\n",
      "11896/11896 [==============================] - 8s 656us/step - loss: 0.4053 - acc: 0.8454 - val_loss: 0.5209 - val_acc: 0.8008\n",
      "Epoch 14/32\n",
      "11896/11896 [==============================] - 8s 649us/step - loss: 0.4041 - acc: 0.8436 - val_loss: 0.5076 - val_acc: 0.8000\n",
      "Epoch 15/32\n",
      "11896/11896 [==============================] - 8s 656us/step - loss: 0.3966 - acc: 0.8526 - val_loss: 0.5143 - val_acc: 0.8039\n",
      "Epoch 16/32\n",
      "11896/11896 [==============================] - 8s 664us/step - loss: 0.3919 - acc: 0.8485 - val_loss: 0.5111 - val_acc: 0.8016\n",
      "Epoch 17/32\n",
      "11896/11896 [==============================] - 8s 653us/step - loss: 0.3895 - acc: 0.8518 - val_loss: 0.5284 - val_acc: 0.8008\n",
      "Epoch 18/32\n",
      "11896/11896 [==============================] - 8s 651us/step - loss: 0.3821 - acc: 0.8545 - val_loss: 0.5153 - val_acc: 0.8047\n",
      "Epoch 19/32\n",
      "11896/11896 [==============================] - 8s 653us/step - loss: 0.3757 - acc: 0.8566 - val_loss: 0.5224 - val_acc: 0.7992\n",
      "Epoch 20/32\n",
      "11896/11896 [==============================] - 8s 656us/step - loss: 0.3825 - acc: 0.8556 - val_loss: 0.5443 - val_acc: 0.7922\n",
      "Epoch 21/32\n",
      "11896/11896 [==============================] - 8s 658us/step - loss: 0.3688 - acc: 0.8579 - val_loss: 0.5273 - val_acc: 0.8000\n",
      "Epoch 22/32\n",
      "11896/11896 [==============================] - 8s 651us/step - loss: 0.3695 - acc: 0.8581 - val_loss: 0.5262 - val_acc: 0.8008\n",
      "Epoch 23/32\n",
      "11896/11896 [==============================] - 8s 660us/step - loss: 0.3577 - acc: 0.8637 - val_loss: 0.5336 - val_acc: 0.8047\n",
      "Epoch 24/32\n",
      "11896/11896 [==============================] - 8s 658us/step - loss: 0.3538 - acc: 0.8643 - val_loss: 0.5433 - val_acc: 0.8008\n",
      "Epoch 25/32\n",
      "11896/11896 [==============================] - 8s 677us/step - loss: 0.3525 - acc: 0.8685 - val_loss: 0.5429 - val_acc: 0.7969\n",
      "Epoch 26/32\n",
      "11896/11896 [==============================] - 8s 652us/step - loss: 0.3481 - acc: 0.8674 - val_loss: 0.5302 - val_acc: 0.8094\n",
      "Epoch 27/32\n",
      "11896/11896 [==============================] - 8s 650us/step - loss: 0.3470 - acc: 0.8670 - val_loss: 0.5483 - val_acc: 0.8008\n",
      "Epoch 28/32\n",
      "11896/11896 [==============================] - 8s 659us/step - loss: 0.3408 - acc: 0.8701 - val_loss: 0.5446 - val_acc: 0.8070\n",
      "Epoch 29/32\n",
      "11896/11896 [==============================] - 8s 659us/step - loss: 0.3320 - acc: 0.8746 - val_loss: 0.5521 - val_acc: 0.8008\n",
      "Epoch 30/32\n",
      "11896/11896 [==============================] - 8s 695us/step - loss: 0.3309 - acc: 0.8755 - val_loss: 0.5581 - val_acc: 0.7937\n",
      "Epoch 31/32\n",
      "11896/11896 [==============================] - 8s 709us/step - loss: 0.3314 - acc: 0.8736 - val_loss: 0.5538 - val_acc: 0.7984\n",
      "Epoch 32/32\n",
      "11896/11896 [==============================] - 8s 676us/step - loss: 0.3318 - acc: 0.8743 - val_loss: 0.5715 - val_acc: 0.7953\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_X, y_train_oh, validation_data=(X_valid, Y_valid), callbacks=callbacks_list, epochs=NB_START_EPOCHS\n",
    "                       , batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.809375"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weight_twitter_embedding.26.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1464/1464 [==============================] - 1s 385us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5831101175214424, 0.8005464484131402]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_X, y_test_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(inp):\n",
    "    punct = ['!','\"', '#', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', \n",
    "         '[', '/', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\n']\n",
    "#     neg = negate_sequence(inp)\n",
    "    for i in punct:\n",
    "        inp.replace(i, '')\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    inp = tokenizer.tokenize(inp)\n",
    "    MAX_SEQ = 20\n",
    "    n = MAX_SEQ - len(inp)\n",
    "    if n < 0:\n",
    "        inp = inp[:MAX_SEQ]\n",
    "    else:\n",
    "        for i in range(n):\n",
    "            inp.append('<PAD>')\n",
    "    for v in range(len(inp)):\n",
    "        if inp[v] not in embeddings_index:\n",
    "            inp[v] = embeddings_index['<UNK>']\n",
    "        else:\n",
    "            inp[v] = embeddings_index[inp[v]]\n",
    "    return np.reshape(np.array(inp) , (1 , 20 , 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def negate_sequence(text):\n",
    "#     negation = False\n",
    "#     tk = TreebankWordTokenizer()\n",
    "#     negs = [\"not\", \"n't\", \"no\"]\n",
    "#     words = tk.tokenize(text)\n",
    "#     for word in words:\n",
    "#         word = word.lower()\n",
    "#         if word in negs:\n",
    "#             negation = not negation\n",
    "#     return negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classify(sent):\n",
    "#     neg = negate_sequence(sent)\n",
    "#     prep = prep_data(sent)\n",
    "#     arr = model.predict(prep)\n",
    "#     print(arr)\n",
    "#     argmax = arr.argmax()\n",
    "#     if argmax == 0 and not neg:\n",
    "#         print(\"negative\")\n",
    "#     elif argmax == 1:\n",
    "#         print(\"neutral\")\n",
    "#     elif argmax == 2:\n",
    "#         print(\"positive\")\n",
    "#     elif argmax == 0 and neg:\n",
    "#         tk = TreebankWordTokenizer()\n",
    "#         words = tk.tokenize(sent)\n",
    "#         negs = [\"not\", \"n't\", \"no\"]\n",
    "#         sent2 = []\n",
    "#         for word in words:\n",
    "#             if word not in negs:\n",
    "#                 sent2.append(word)\n",
    "#         new = ' '.join(sent2)\n",
    "#         new_prep = prep_data(new)\n",
    "#         new_argmax = model.predict(new_prep).argmax()\n",
    "#         if new_argmax == argmax:\n",
    "#             print('positive')\n",
    "#         else:\n",
    "#             print('negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7832205 , 0.04439801, 0.17238142]], dtype=float32)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"our airplane's door was not working and we were stucked in there for hours\"\n",
    "model.predict(prep_data(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
